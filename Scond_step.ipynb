{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WORKING ON COMBING MULTIPLE LEAD FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "from natsort import natsorted\n",
    "\n",
    "#creating list to store file_names\n",
    "NORMAL_=[]\n",
    "MI_=[]\n",
    "PMI_=[]\n",
    "HB_=[]\n",
    "\n",
    "normal = \"C:\\\\Users\\\\vargh\\\\Desktop\\\\Easy_G\\\\data_csv\\\\Normal heartbeat\"\n",
    "abnormal = \"C:\\\\Users\\\\vargh\\\\Desktop\\\\Easy_G\\\\data_csv\\\\Abnormal heartbeat\"\n",
    "MI = \"C:\\\\Users\\\\vargh\\\\Desktop\\\\Easy_G\\\\data_csv\\\\Myocardial Infarction\"\n",
    "MI_history = \"C:\\\\Users\\\\vargh\\\\Desktop\\\\Easy_G\\\\data_csv\\\\History of MI\"\n",
    "\n",
    "Types_ECG = {'normal':normal,'Abnormal_hear_beat':abnormal,'MI':MI,'History_MI':MI_history}\n",
    "\n",
    "for types,folder in Types_ECG.items():\n",
    "  for files in os.listdir(folder):\n",
    "    if types=='normal':\n",
    "      NORMAL_.append(files)\n",
    "    elif types=='Abnormal_hear_beat':\n",
    "      HB_.append(files)\n",
    "    elif types=='MI':\n",
    "      MI_.append(files)\n",
    "    elif types=='History_MI':\n",
    "      PMI_.append(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaled_data_1D_1.csv',\n",
       " 'scaled_data_1D_10.csv',\n",
       " 'scaled_data_1D_11.csv',\n",
       " 'scaled_data_1D_12.csv',\n",
       " 'scaled_data_1D_13.csv',\n",
       " 'scaled_data_1D_2.csv',\n",
       " 'scaled_data_1D_3.csv',\n",
       " 'scaled_data_1D_4.csv',\n",
       " 'scaled_data_1D_5.csv',\n",
       " 'scaled_data_1D_6.csv',\n",
       " 'scaled_data_1D_7.csv',\n",
       " 'scaled_data_1D_8.csv',\n",
       " 'scaled_data_1D_9.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NORMAL_.sort()\n",
    "NORMAL_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaled_data_1D_1.csv',\n",
       " 'scaled_data_1D_10.csv',\n",
       " 'scaled_data_1D_11.csv',\n",
       " 'scaled_data_1D_12.csv',\n",
       " 'scaled_data_1D_13.csv',\n",
       " 'scaled_data_1D_2.csv',\n",
       " 'scaled_data_1D_3.csv',\n",
       " 'scaled_data_1D_4.csv',\n",
       " 'scaled_data_1D_5.csv',\n",
       " 'scaled_data_1D_6.csv',\n",
       " 'scaled_data_1D_7.csv',\n",
       " 'scaled_data_1D_8.csv',\n",
       " 'scaled_data_1D_9.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MI_.sort()\n",
    "MI_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaled_data_1D_1.csv',\n",
       " 'scaled_data_1D_10.csv',\n",
       " 'scaled_data_1D_11.csv',\n",
       " 'scaled_data_1D_12.csv',\n",
       " 'scaled_data_1D_13.csv',\n",
       " 'scaled_data_1D_2.csv',\n",
       " 'scaled_data_1D_3.csv',\n",
       " 'scaled_data_1D_4.csv',\n",
       " 'scaled_data_1D_5.csv',\n",
       " 'scaled_data_1D_6.csv',\n",
       " 'scaled_data_1D_7.csv',\n",
       " 'scaled_data_1D_8.csv',\n",
       " 'scaled_data_1D_9.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PMI_.sort()\n",
    "PMI_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaled_data_1D_1.csv',\n",
       " 'scaled_data_1D_10.csv',\n",
       " 'scaled_data_1D_11.csv',\n",
       " 'scaled_data_1D_12.csv',\n",
       " 'scaled_data_1D_13.csv',\n",
       " 'scaled_data_1D_2.csv',\n",
       " 'scaled_data_1D_3.csv',\n",
       " 'scaled_data_1D_4.csv',\n",
       " 'scaled_data_1D_5.csv',\n",
       " 'scaled_data_1D_6.csv',\n",
       " 'scaled_data_1D_7.csv',\n",
       " 'scaled_data_1D_8.csv',\n",
       " 'scaled_data_1D_9.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HB_.sort()\n",
    "HB_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMBINED CSV OF EACH LEAD(1-12) FROM ALL IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop over and create combined csv files for each leads.\n",
    "for x in range(len(MI_)):\n",
    "  df1=pd.read_csv(\"C:\\\\Users\\\\vargh\\\\Desktop\\\\Easy_G\\\\data_csv\\\\Normal heartbeat\\\\{}\".format(NORMAL_[x]))\n",
    "  df2=pd.read_csv(\"C:\\\\Users\\\\vargh\\\\Desktop\\\\Easy_G\\\\data_csv\\\\Abnormal heartbeat\\\\{}\".format(HB_[x]))\n",
    "  df3=pd.read_csv(\"C:\\\\Users\\\\vargh\\\\Desktop\\\\Easy_G\\\\data_csv\\\\Myocardial Infarction\\\\{}\".format(MI_[x]))\n",
    "  df4=pd.read_csv(\"C:\\\\Users\\\\vargh\\\\Desktop\\\\Easy_G\\\\data_csv\\\\History of MI\\\\{}\".format(PMI_[x]))\n",
    "  final_df = pd.concat([df1,df2,df3,df4],ignore_index=True)\n",
    "  final_df.to_csv(\"C:\\\\Users\\\\vargh\\\\Desktop\\\\Easy_G\\\\Combined_IDLead_{}.csv\".format(x+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['No', 'HB', 'MI', 'PM'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now reading just lead1\n",
    "df=pd.read_csv(\"C:\\\\Users\\\\vargh\\\\Desktop\\\\Easy_G\\\\Combined_IDLead_1.csv\")\n",
    "df['Target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.963402</td>\n",
       "      <td>0.966190</td>\n",
       "      <td>0.929274</td>\n",
       "      <td>0.865036</td>\n",
       "      <td>0.794078</td>\n",
       "      <td>0.704745</td>\n",
       "      <td>0.611444</td>\n",
       "      <td>0.513448</td>\n",
       "      <td>0.423232</td>\n",
       "      <td>0.368119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.309006</td>\n",
       "      <td>0.400056</td>\n",
       "      <td>0.495693</td>\n",
       "      <td>0.592619</td>\n",
       "      <td>0.682971</td>\n",
       "      <td>0.780323</td>\n",
       "      <td>0.865936</td>\n",
       "      <td>0.918180</td>\n",
       "      <td>0.925146</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.541375</td>\n",
       "      <td>0.581143</td>\n",
       "      <td>0.620171</td>\n",
       "      <td>0.642222</td>\n",
       "      <td>0.669337</td>\n",
       "      <td>0.689173</td>\n",
       "      <td>0.766609</td>\n",
       "      <td>0.876878</td>\n",
       "      <td>0.942333</td>\n",
       "      <td>0.848699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.481424</td>\n",
       "      <td>0.635611</td>\n",
       "      <td>0.722129</td>\n",
       "      <td>0.642143</td>\n",
       "      <td>0.584403</td>\n",
       "      <td>0.547887</td>\n",
       "      <td>0.505982</td>\n",
       "      <td>0.458419</td>\n",
       "      <td>0.412658</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.933871</td>\n",
       "      <td>0.933871</td>\n",
       "      <td>0.933871</td>\n",
       "      <td>0.933860</td>\n",
       "      <td>0.929573</td>\n",
       "      <td>0.902375</td>\n",
       "      <td>0.902041</td>\n",
       "      <td>0.869612</td>\n",
       "      <td>0.828913</td>\n",
       "      <td>0.814614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.731199</td>\n",
       "      <td>0.777451</td>\n",
       "      <td>0.812820</td>\n",
       "      <td>0.828121</td>\n",
       "      <td>0.843195</td>\n",
       "      <td>0.833822</td>\n",
       "      <td>0.844328</td>\n",
       "      <td>0.844897</td>\n",
       "      <td>0.844897</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.840359</td>\n",
       "      <td>0.847049</td>\n",
       "      <td>0.852590</td>\n",
       "      <td>0.860626</td>\n",
       "      <td>0.857754</td>\n",
       "      <td>0.848815</td>\n",
       "      <td>0.819930</td>\n",
       "      <td>0.760774</td>\n",
       "      <td>0.691579</td>\n",
       "      <td>0.611855</td>\n",
       "      <td>...</td>\n",
       "      <td>0.566292</td>\n",
       "      <td>0.646234</td>\n",
       "      <td>0.723609</td>\n",
       "      <td>0.784745</td>\n",
       "      <td>0.810104</td>\n",
       "      <td>0.817961</td>\n",
       "      <td>0.817854</td>\n",
       "      <td>0.809803</td>\n",
       "      <td>0.799044</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.894249</td>\n",
       "      <td>0.799704</td>\n",
       "      <td>0.674077</td>\n",
       "      <td>0.664225</td>\n",
       "      <td>0.775689</td>\n",
       "      <td>0.861098</td>\n",
       "      <td>0.877680</td>\n",
       "      <td>0.876781</td>\n",
       "      <td>0.853368</td>\n",
       "      <td>0.834667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.765732</td>\n",
       "      <td>0.659982</td>\n",
       "      <td>0.543587</td>\n",
       "      <td>0.413399</td>\n",
       "      <td>0.296027</td>\n",
       "      <td>0.168565</td>\n",
       "      <td>0.067618</td>\n",
       "      <td>0.082788</td>\n",
       "      <td>0.191967</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>0.829815</td>\n",
       "      <td>0.832084</td>\n",
       "      <td>0.852396</td>\n",
       "      <td>0.909665</td>\n",
       "      <td>0.988242</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923323</td>\n",
       "      <td>0.821865</td>\n",
       "      <td>0.721302</td>\n",
       "      <td>0.612039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.429721</td>\n",
       "      <td>0.531567</td>\n",
       "      <td>0.642137</td>\n",
       "      <td>0.742063</td>\n",
       "      <td>0.833042</td>\n",
       "      <td>0.814867</td>\n",
       "      <td>0.777622</td>\n",
       "      <td>0.760714</td>\n",
       "      <td>0.759294</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>0.821040</td>\n",
       "      <td>0.803252</td>\n",
       "      <td>0.802413</td>\n",
       "      <td>0.808188</td>\n",
       "      <td>0.809180</td>\n",
       "      <td>0.796091</td>\n",
       "      <td>0.779687</td>\n",
       "      <td>0.778686</td>\n",
       "      <td>0.777388</td>\n",
       "      <td>0.796759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.733465</td>\n",
       "      <td>0.725209</td>\n",
       "      <td>0.729679</td>\n",
       "      <td>0.748875</td>\n",
       "      <td>0.758765</td>\n",
       "      <td>0.759886</td>\n",
       "      <td>0.757626</td>\n",
       "      <td>0.748524</td>\n",
       "      <td>0.759931</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>0.834846</td>\n",
       "      <td>0.835026</td>\n",
       "      <td>0.848415</td>\n",
       "      <td>0.850183</td>\n",
       "      <td>0.852194</td>\n",
       "      <td>0.850544</td>\n",
       "      <td>0.852060</td>\n",
       "      <td>0.881185</td>\n",
       "      <td>0.913712</td>\n",
       "      <td>0.940417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.775646</td>\n",
       "      <td>0.731681</td>\n",
       "      <td>0.723865</td>\n",
       "      <td>0.718354</td>\n",
       "      <td>0.728084</td>\n",
       "      <td>0.726277</td>\n",
       "      <td>0.690386</td>\n",
       "      <td>0.701059</td>\n",
       "      <td>0.699635</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>0.793255</td>\n",
       "      <td>0.703758</td>\n",
       "      <td>0.586116</td>\n",
       "      <td>0.458812</td>\n",
       "      <td>0.332085</td>\n",
       "      <td>0.216252</td>\n",
       "      <td>0.189161</td>\n",
       "      <td>0.293799</td>\n",
       "      <td>0.399837</td>\n",
       "      <td>0.506746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.235414</td>\n",
       "      <td>0.122761</td>\n",
       "      <td>0.045529</td>\n",
       "      <td>0.033899</td>\n",
       "      <td>0.136884</td>\n",
       "      <td>0.253815</td>\n",
       "      <td>0.371475</td>\n",
       "      <td>0.487262</td>\n",
       "      <td>0.601039</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>0.929683</td>\n",
       "      <td>0.934702</td>\n",
       "      <td>0.922929</td>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.770322</td>\n",
       "      <td>0.674787</td>\n",
       "      <td>0.572909</td>\n",
       "      <td>0.491908</td>\n",
       "      <td>0.405543</td>\n",
       "      <td>0.378534</td>\n",
       "      <td>...</td>\n",
       "      <td>0.287430</td>\n",
       "      <td>0.395687</td>\n",
       "      <td>0.503943</td>\n",
       "      <td>0.610437</td>\n",
       "      <td>0.706004</td>\n",
       "      <td>0.800673</td>\n",
       "      <td>0.854129</td>\n",
       "      <td>0.867868</td>\n",
       "      <td>0.841508</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>928 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    0.963402  0.966190  0.929274  0.865036  0.794078  0.704745  0.611444   \n",
       "1    0.541375  0.581143  0.620171  0.642222  0.669337  0.689173  0.766609   \n",
       "2    0.933871  0.933871  0.933871  0.933860  0.929573  0.902375  0.902041   \n",
       "3    0.840359  0.847049  0.852590  0.860626  0.857754  0.848815  0.819930   \n",
       "4    0.894249  0.799704  0.674077  0.664225  0.775689  0.861098  0.877680   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "923  0.829815  0.832084  0.852396  0.909665  0.988242  1.000000  0.923323   \n",
       "924  0.821040  0.803252  0.802413  0.808188  0.809180  0.796091  0.779687   \n",
       "925  0.834846  0.835026  0.848415  0.850183  0.852194  0.850544  0.852060   \n",
       "926  0.793255  0.703758  0.586116  0.458812  0.332085  0.216252  0.189161   \n",
       "927  0.929683  0.934702  0.922929  0.858824  0.770322  0.674787  0.572909   \n",
       "\n",
       "            7         8         9  ...       246       247       248  \\\n",
       "0    0.513448  0.423232  0.368119  ...  0.309006  0.400056  0.495693   \n",
       "1    0.876878  0.942333  0.848699  ...  0.481424  0.635611  0.722129   \n",
       "2    0.869612  0.828913  0.814614  ...  0.731199  0.777451  0.812820   \n",
       "3    0.760774  0.691579  0.611855  ...  0.566292  0.646234  0.723609   \n",
       "4    0.876781  0.853368  0.834667  ...  0.765732  0.659982  0.543587   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "923  0.821865  0.721302  0.612039  ...  0.429721  0.531567  0.642137   \n",
       "924  0.778686  0.777388  0.796759  ...  0.733465  0.725209  0.729679   \n",
       "925  0.881185  0.913712  0.940417  ...  0.775646  0.731681  0.723865   \n",
       "926  0.293799  0.399837  0.506746  ...  0.235414  0.122761  0.045529   \n",
       "927  0.491908  0.405543  0.378534  ...  0.287430  0.395687  0.503943   \n",
       "\n",
       "          249       250       251       252       253       254  target  \n",
       "0    0.592619  0.682971  0.780323  0.865936  0.918180  0.925146       2  \n",
       "1    0.642143  0.584403  0.547887  0.505982  0.458419  0.412658       2  \n",
       "2    0.828121  0.843195  0.833822  0.844328  0.844897  0.844897       2  \n",
       "3    0.784745  0.810104  0.817961  0.817854  0.809803  0.799044       2  \n",
       "4    0.413399  0.296027  0.168565  0.067618  0.082788  0.191967       2  \n",
       "..        ...       ...       ...       ...       ...       ...     ...  \n",
       "923  0.742063  0.833042  0.814867  0.777622  0.760714  0.759294       3  \n",
       "924  0.748875  0.758765  0.759886  0.757626  0.748524  0.759931       3  \n",
       "925  0.718354  0.728084  0.726277  0.690386  0.701059  0.699635       3  \n",
       "926  0.033899  0.136884  0.253815  0.371475  0.487262  0.601039       3  \n",
       "927  0.610437  0.706004  0.800673  0.854129  0.867868  0.841508       3  \n",
       "\n",
       "[928 rows x 256 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert Target column values as Numeric using ngroups\n",
    "encode_target_label = df.groupby('Target').ngroup().rename(\"target\").to_frame()\n",
    "test_final  = df.merge(encode_target_label, left_index=True, right_index=True)\n",
    "test_final.drop(columns=['Target'],inplace=True)\n",
    "test_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PERFORM DIMENSIONALITY REDUCTION JUST FOR CHECKING/UNDERSTANDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of each component: [1.76631711e-01 9.47841846e-02 7.00100519e-02 6.14026654e-02\n",
      " 5.33383989e-02 4.23623127e-02 3.69098118e-02 3.38745152e-02\n",
      " 3.00158981e-02 2.89945688e-02 2.64691913e-02 2.42569640e-02\n",
      " 2.11690566e-02 1.99633083e-02 1.77748538e-02 1.62162835e-02\n",
      " 1.52721200e-02 1.48309040e-02 1.34418063e-02 1.20062516e-02\n",
      " 1.17530896e-02 1.05531342e-02 9.63050847e-03 9.41801238e-03\n",
      " 8.64223009e-03 8.47025185e-03 7.91878283e-03 7.30862520e-03\n",
      " 6.84760694e-03 6.38087529e-03 5.98449751e-03 5.47038359e-03\n",
      " 5.29039352e-03 4.97484057e-03 4.76542228e-03 4.46934342e-03\n",
      " 4.23682780e-03 4.00132225e-03 3.82983831e-03 3.52568125e-03\n",
      " 3.38250175e-03 3.25792933e-03 3.08591094e-03 3.01371361e-03\n",
      " 2.73110840e-03 2.50310601e-03 2.36348041e-03 2.27449529e-03\n",
      " 2.18960064e-03 1.99106312e-03 1.73887639e-03 1.70320722e-03\n",
      " 1.57079343e-03 1.50915169e-03 1.38122660e-03 1.32580398e-03\n",
      " 1.26115946e-03 1.18773619e-03 1.17785245e-03 1.09298665e-03\n",
      " 1.02856003e-03 9.21386903e-04 7.92491113e-04 7.71182408e-04\n",
      " 6.49755057e-04 6.22135582e-04 6.05545238e-04 5.49402226e-04\n",
      " 5.33573428e-04 4.83661467e-04 4.74732127e-04 4.49183909e-04\n",
      " 4.32753522e-04 4.09160740e-04 3.95604747e-04 3.71414688e-04\n",
      " 3.55188514e-04 3.37060905e-04 3.05051438e-04 2.96587369e-04\n",
      " 2.79139939e-04 2.64923182e-04 2.45988872e-04 2.37435581e-04\n",
      " 2.13299791e-04 1.99890649e-04 1.95898662e-04 1.86587210e-04\n",
      " 1.66963124e-04 1.62158585e-04 1.56780527e-04 1.40730179e-04\n",
      " 1.25012350e-04 1.21310077e-04 1.13750417e-04 1.09384624e-04\n",
      " 1.02563095e-04 9.96687099e-05 9.71423889e-05 9.08978171e-05]\n",
      "\n",
      " Total Variance Explained: 99.8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.349042</td>\n",
       "      <td>-2.463929</td>\n",
       "      <td>1.349728</td>\n",
       "      <td>-0.274637</td>\n",
       "      <td>0.172031</td>\n",
       "      <td>-0.152916</td>\n",
       "      <td>0.821459</td>\n",
       "      <td>-0.098088</td>\n",
       "      <td>-1.356674</td>\n",
       "      <td>-0.221998</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055514</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.039985</td>\n",
       "      <td>0.013171</td>\n",
       "      <td>0.013063</td>\n",
       "      <td>0.020913</td>\n",
       "      <td>0.017657</td>\n",
       "      <td>0.034371</td>\n",
       "      <td>0.035954</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.082457</td>\n",
       "      <td>0.419848</td>\n",
       "      <td>-0.373862</td>\n",
       "      <td>-0.017678</td>\n",
       "      <td>-0.475013</td>\n",
       "      <td>0.472410</td>\n",
       "      <td>-0.089024</td>\n",
       "      <td>0.241118</td>\n",
       "      <td>0.080011</td>\n",
       "      <td>0.072828</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021330</td>\n",
       "      <td>0.069549</td>\n",
       "      <td>-0.018458</td>\n",
       "      <td>0.011811</td>\n",
       "      <td>0.068416</td>\n",
       "      <td>-0.021316</td>\n",
       "      <td>-0.076112</td>\n",
       "      <td>0.037294</td>\n",
       "      <td>0.018213</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.228324</td>\n",
       "      <td>1.172645</td>\n",
       "      <td>-0.524623</td>\n",
       "      <td>0.303817</td>\n",
       "      <td>-0.000940</td>\n",
       "      <td>0.554329</td>\n",
       "      <td>0.218682</td>\n",
       "      <td>-0.507440</td>\n",
       "      <td>-0.524708</td>\n",
       "      <td>-1.222993</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017499</td>\n",
       "      <td>0.048807</td>\n",
       "      <td>0.035660</td>\n",
       "      <td>0.029679</td>\n",
       "      <td>0.009058</td>\n",
       "      <td>-0.049888</td>\n",
       "      <td>-0.018328</td>\n",
       "      <td>-0.038957</td>\n",
       "      <td>0.020283</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.852269</td>\n",
       "      <td>0.621011</td>\n",
       "      <td>0.801915</td>\n",
       "      <td>1.339401</td>\n",
       "      <td>-0.849406</td>\n",
       "      <td>-1.500549</td>\n",
       "      <td>1.074489</td>\n",
       "      <td>1.071347</td>\n",
       "      <td>0.307250</td>\n",
       "      <td>0.134665</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018378</td>\n",
       "      <td>0.004651</td>\n",
       "      <td>-0.023797</td>\n",
       "      <td>-0.007258</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.016112</td>\n",
       "      <td>0.049887</td>\n",
       "      <td>0.003439</td>\n",
       "      <td>0.028685</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.038220</td>\n",
       "      <td>-1.476702</td>\n",
       "      <td>-0.942818</td>\n",
       "      <td>-0.519393</td>\n",
       "      <td>0.313444</td>\n",
       "      <td>0.009207</td>\n",
       "      <td>-0.505386</td>\n",
       "      <td>-0.626590</td>\n",
       "      <td>0.189309</td>\n",
       "      <td>-0.812674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043562</td>\n",
       "      <td>-0.090890</td>\n",
       "      <td>-0.007917</td>\n",
       "      <td>0.046602</td>\n",
       "      <td>-0.024675</td>\n",
       "      <td>0.034068</td>\n",
       "      <td>0.037435</td>\n",
       "      <td>0.005246</td>\n",
       "      <td>-0.000632</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>-0.865127</td>\n",
       "      <td>-0.046079</td>\n",
       "      <td>0.937729</td>\n",
       "      <td>0.310634</td>\n",
       "      <td>-0.456238</td>\n",
       "      <td>-0.378867</td>\n",
       "      <td>1.068905</td>\n",
       "      <td>0.782661</td>\n",
       "      <td>0.659660</td>\n",
       "      <td>1.011713</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026952</td>\n",
       "      <td>-0.103273</td>\n",
       "      <td>-0.031672</td>\n",
       "      <td>-0.010351</td>\n",
       "      <td>-0.020019</td>\n",
       "      <td>0.039171</td>\n",
       "      <td>-0.039711</td>\n",
       "      <td>0.045305</td>\n",
       "      <td>0.008719</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>-0.647144</td>\n",
       "      <td>1.731302</td>\n",
       "      <td>0.737931</td>\n",
       "      <td>-0.076732</td>\n",
       "      <td>0.066095</td>\n",
       "      <td>-0.406027</td>\n",
       "      <td>-0.302324</td>\n",
       "      <td>0.081770</td>\n",
       "      <td>-0.083183</td>\n",
       "      <td>-1.064898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012981</td>\n",
       "      <td>-0.011433</td>\n",
       "      <td>0.017333</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>-0.007625</td>\n",
       "      <td>-0.008002</td>\n",
       "      <td>-0.023954</td>\n",
       "      <td>-0.021727</td>\n",
       "      <td>0.003103</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>-1.102371</td>\n",
       "      <td>1.278477</td>\n",
       "      <td>-0.331359</td>\n",
       "      <td>-0.922451</td>\n",
       "      <td>0.158848</td>\n",
       "      <td>0.131820</td>\n",
       "      <td>0.004359</td>\n",
       "      <td>-0.547190</td>\n",
       "      <td>0.617257</td>\n",
       "      <td>-0.391317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064916</td>\n",
       "      <td>-0.012409</td>\n",
       "      <td>0.061544</td>\n",
       "      <td>0.012439</td>\n",
       "      <td>-0.076973</td>\n",
       "      <td>-0.033556</td>\n",
       "      <td>-0.024479</td>\n",
       "      <td>-0.024938</td>\n",
       "      <td>-0.001274</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>-0.276794</td>\n",
       "      <td>-0.306218</td>\n",
       "      <td>1.012962</td>\n",
       "      <td>-0.827206</td>\n",
       "      <td>0.380327</td>\n",
       "      <td>1.570077</td>\n",
       "      <td>0.954821</td>\n",
       "      <td>-0.816475</td>\n",
       "      <td>-0.624750</td>\n",
       "      <td>1.503205</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005155</td>\n",
       "      <td>0.027698</td>\n",
       "      <td>0.010969</td>\n",
       "      <td>-0.040216</td>\n",
       "      <td>0.056364</td>\n",
       "      <td>0.042180</td>\n",
       "      <td>0.014574</td>\n",
       "      <td>-0.020328</td>\n",
       "      <td>-0.016525</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>-1.631759</td>\n",
       "      <td>0.586710</td>\n",
       "      <td>1.745229</td>\n",
       "      <td>-0.184511</td>\n",
       "      <td>-1.256110</td>\n",
       "      <td>0.661976</td>\n",
       "      <td>0.903205</td>\n",
       "      <td>0.185535</td>\n",
       "      <td>0.257371</td>\n",
       "      <td>0.549247</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018663</td>\n",
       "      <td>0.070827</td>\n",
       "      <td>-0.009597</td>\n",
       "      <td>0.032205</td>\n",
       "      <td>-0.025574</td>\n",
       "      <td>-0.018085</td>\n",
       "      <td>0.027823</td>\n",
       "      <td>-0.004783</td>\n",
       "      <td>0.021073</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>928 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0   -0.349042 -2.463929  1.349728 -0.274637  0.172031 -0.152916  0.821459   \n",
       "1    2.082457  0.419848 -0.373862 -0.017678 -0.475013  0.472410 -0.089024   \n",
       "2   -2.228324  1.172645 -0.524623  0.303817 -0.000940  0.554329  0.218682   \n",
       "3   -0.852269  0.621011  0.801915  1.339401 -0.849406 -1.500549  1.074489   \n",
       "4   -0.038220 -1.476702 -0.942818 -0.519393  0.313444  0.009207 -0.505386   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "923 -0.865127 -0.046079  0.937729  0.310634 -0.456238 -0.378867  1.068905   \n",
       "924 -0.647144  1.731302  0.737931 -0.076732  0.066095 -0.406027 -0.302324   \n",
       "925 -1.102371  1.278477 -0.331359 -0.922451  0.158848  0.131820  0.004359   \n",
       "926 -0.276794 -0.306218  1.012962 -0.827206  0.380327  1.570077  0.954821   \n",
       "927 -1.631759  0.586710  1.745229 -0.184511 -1.256110  0.661976  0.903205   \n",
       "\n",
       "            7         8         9  ...        91        92        93  \\\n",
       "0   -0.098088 -1.356674 -0.221998  ... -0.055514  0.000326  0.039985   \n",
       "1    0.241118  0.080011  0.072828  ... -0.021330  0.069549 -0.018458   \n",
       "2   -0.507440 -0.524708 -1.222993  ... -0.017499  0.048807  0.035660   \n",
       "3    1.071347  0.307250  0.134665  ... -0.018378  0.004651 -0.023797   \n",
       "4   -0.626590  0.189309 -0.812674  ...  0.043562 -0.090890 -0.007917   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "923  0.782661  0.659660  1.011713  ...  0.026952 -0.103273 -0.031672   \n",
       "924  0.081770 -0.083183 -1.064898  ...  0.012981 -0.011433  0.017333   \n",
       "925 -0.547190  0.617257 -0.391317  ...  0.064916 -0.012409  0.061544   \n",
       "926 -0.816475 -0.624750  1.503205  ... -0.005155  0.027698  0.010969   \n",
       "927  0.185535  0.257371  0.549247  ...  0.018663  0.070827 -0.009597   \n",
       "\n",
       "           94        95        96        97        98        99  target  \n",
       "0    0.013171  0.013063  0.020913  0.017657  0.034371  0.035954       2  \n",
       "1    0.011811  0.068416 -0.021316 -0.076112  0.037294  0.018213       2  \n",
       "2    0.029679  0.009058 -0.049888 -0.018328 -0.038957  0.020283       2  \n",
       "3   -0.007258  0.014064  0.016112  0.049887  0.003439  0.028685       2  \n",
       "4    0.046602 -0.024675  0.034068  0.037435  0.005246 -0.000632       2  \n",
       "..        ...       ...       ...       ...       ...       ...     ...  \n",
       "923 -0.010351 -0.020019  0.039171 -0.039711  0.045305  0.008719       3  \n",
       "924  0.039216 -0.007625 -0.008002 -0.023954 -0.021727  0.003103       3  \n",
       "925  0.012439 -0.076973 -0.033556 -0.024479 -0.024938 -0.001274       3  \n",
       "926 -0.040216  0.056364  0.042180  0.014574 -0.020328 -0.016525       3  \n",
       "927  0.032205 -0.025574 -0.018085  0.027823 -0.004783  0.021073       3  \n",
       "\n",
       "[928 rows x 101 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just for testing\n",
    "# Now Perform Dimensionality reduction (PCA) on that Dataframe and check\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#do PCA and choose componeents as 100\n",
    "pca = PCA(n_components=100)\n",
    "x_pca = pca.fit_transform(test_final.iloc[:,0:-1])\n",
    "x_pca = pd.DataFrame(x_pca)\n",
    "\n",
    "# Calculate the variance explained by priciple components\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print('Variance of each component:', pca.explained_variance_ratio_)\n",
    "print('\\n Total Variance Explained:', round(sum(list(pca.explained_variance_ratio_))*100, 2))\n",
    "\n",
    "#store the new pca generated dimensions in a dataframe\n",
    "pca_df = pd.DataFrame(data = x_pca)\n",
    "target = pd.Series(test_final['target'], name='target')\n",
    "result_df = pd.concat([pca_df, target], axis=1)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRYING DIFFERENT ML MODELS ON A SINGLE LEAD(EX : 1) POST DIMENSIONALITY REDUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7983870967741935\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.69      0.75       105\n",
      "           1       0.94      1.00      0.97        94\n",
      "           2       0.82      0.77      0.79       112\n",
      "           3       0.56      0.74      0.63        61\n",
      "\n",
      "    accuracy                           0.80       372\n",
      "   macro avg       0.79      0.80      0.79       372\n",
      "weighted avg       0.81      0.80      0.80       372\n",
      "\n",
      "Tuned Model Parameters: {'knn__n_neighbors': 1}\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules for ML model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Setup the pipeline steps:\n",
    "steps = [('knn', KNeighborsClassifier())]\n",
    "        \n",
    "# Create the pipeline: pipeline \n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# have paased less range value of hyperparamter since i'm using free tier version of google colab.\n",
    "k_range = list(range(1, 9))\n",
    "parameters = dict(knn__n_neighbors=k_range)\n",
    "\n",
    "#input\n",
    "X = result_df.iloc[:,0:-1]\n",
    "\n",
    "#target\n",
    "y=result_df.iloc[:,-1]\n",
    "\n",
    "# Create train and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n",
    "\n",
    "#increasing cv score takes lot of time in gooogle colab, so kept it just 2.\n",
    "cv = GridSearchCV(pipeline,parameters,cv=2)\n",
    "\n",
    "cv.fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = cv.predict(X_test)\n",
    "\n",
    "Knn_Accuracy = cv.score(X_test, y_test)\n",
    "\n",
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(Knn_Accuracy))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5913978494623656\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.40      0.46       105\n",
      "           1       0.69      1.00      0.82        94\n",
      "           2       0.63      0.55      0.59       112\n",
      "           3       0.38      0.36      0.37        61\n",
      "\n",
      "    accuracy                           0.59       372\n",
      "   macro avg       0.56      0.58      0.56       372\n",
      "weighted avg       0.58      0.59      0.57       372\n",
      "\n",
      "Tuned Model Parameters: {'lr__C': 10000.0, 'lr__penalty': 'l2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Setup the pipeline steps:\n",
    "steps = [('lr', LogisticRegression())]\n",
    "        \n",
    "# Create the pipeline: pipeline \n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "#input\n",
    "X = result_df.iloc[:,0:-1]\n",
    "\n",
    "#target\n",
    "y=result_df.iloc[:,-1]\n",
    "\n",
    "#parameters for gridsearchcv if we increase range of entries from 5 to higher value, we can get greater accurange\n",
    "c_space = np.logspace(-4, 4, 3)\n",
    "parameters = {'lr__C': c_space,'lr__penalty': ['l2']} \n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n",
    "\n",
    "#call GridSearchCV and set crossvalscore to 2\n",
    "cv = GridSearchCV(pipeline,parameters,cv=2)\n",
    "\n",
    "cv.fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = cv.predict(X_test)\n",
    "LR_Accuracy = cv.score(X_test, y_test)\n",
    "\n",
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(LR_Accuracy))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7956989247311828\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      1.00      0.72        93\n",
      "           1       1.00      1.00      1.00        99\n",
      "           2       0.97      0.54      0.69       117\n",
      "           3       1.00      0.65      0.79        63\n",
      "\n",
      "    accuracy                           0.80       372\n",
      "   macro avg       0.88      0.80      0.80       372\n",
      "weighted avg       0.88      0.80      0.80       372\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules for ML model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Setup the pipeline\n",
    "steps = [('SVM', SVC())]\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "#input\n",
    "X = result_df.iloc[:,0:-1]\n",
    "\n",
    "#target\n",
    "y=result_df.iloc[:,-1]\n",
    "\n",
    "# Specify the hyperparameter space, if we increase the penalty(c) and gamma value the accurancy can be increased.\n",
    "#since it takes lots of time in google colab provided only a single value\n",
    "parameters = {'SVM__C':[10],'SVM__gamma':[1]}\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=21)\n",
    "\n",
    "cv = GridSearchCV(pipeline,parameters,cv=3)\n",
    "cv.fit(X_train,y_train)\n",
    "\n",
    "y_pred = cv.predict(X_test)\n",
    "SVM_Accuracy = cv.score(X_test, y_test)\n",
    "\n",
    "# Compute and print metrics\n",
    "SVM_Accuracy=cv.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy: {}\".format(SVM_Accuracy))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOW COMBINING ALL 12 LEADS INTO A SINGLE CSV FILE AND THEN PERFROM MODEL ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3050</th>\n",
       "      <th>3051</th>\n",
       "      <th>3052</th>\n",
       "      <th>3053</th>\n",
       "      <th>3054</th>\n",
       "      <th>3055</th>\n",
       "      <th>3056</th>\n",
       "      <th>3057</th>\n",
       "      <th>3058</th>\n",
       "      <th>3059</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.963402</td>\n",
       "      <td>0.966190</td>\n",
       "      <td>0.929274</td>\n",
       "      <td>0.865036</td>\n",
       "      <td>0.794078</td>\n",
       "      <td>0.704745</td>\n",
       "      <td>0.611444</td>\n",
       "      <td>0.513448</td>\n",
       "      <td>0.423232</td>\n",
       "      <td>0.368119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.527587</td>\n",
       "      <td>0.600228</td>\n",
       "      <td>0.668378</td>\n",
       "      <td>0.732227</td>\n",
       "      <td>0.761643</td>\n",
       "      <td>0.749170</td>\n",
       "      <td>0.732649</td>\n",
       "      <td>0.754679</td>\n",
       "      <td>0.762857</td>\n",
       "      <td>0.754203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.541375</td>\n",
       "      <td>0.581143</td>\n",
       "      <td>0.620171</td>\n",
       "      <td>0.642222</td>\n",
       "      <td>0.669337</td>\n",
       "      <td>0.689173</td>\n",
       "      <td>0.766609</td>\n",
       "      <td>0.876878</td>\n",
       "      <td>0.942333</td>\n",
       "      <td>0.848699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.465567</td>\n",
       "      <td>0.467715</td>\n",
       "      <td>0.549389</td>\n",
       "      <td>0.631177</td>\n",
       "      <td>0.690901</td>\n",
       "      <td>0.695485</td>\n",
       "      <td>0.662535</td>\n",
       "      <td>0.647105</td>\n",
       "      <td>0.659347</td>\n",
       "      <td>0.651568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.933871</td>\n",
       "      <td>0.933871</td>\n",
       "      <td>0.933871</td>\n",
       "      <td>0.933860</td>\n",
       "      <td>0.929573</td>\n",
       "      <td>0.902375</td>\n",
       "      <td>0.902041</td>\n",
       "      <td>0.869612</td>\n",
       "      <td>0.828913</td>\n",
       "      <td>0.814614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.949801</td>\n",
       "      <td>0.936326</td>\n",
       "      <td>0.922356</td>\n",
       "      <td>0.929721</td>\n",
       "      <td>0.924299</td>\n",
       "      <td>0.922992</td>\n",
       "      <td>0.924178</td>\n",
       "      <td>0.932267</td>\n",
       "      <td>0.922647</td>\n",
       "      <td>0.899251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.840359</td>\n",
       "      <td>0.847049</td>\n",
       "      <td>0.852590</td>\n",
       "      <td>0.860626</td>\n",
       "      <td>0.857754</td>\n",
       "      <td>0.848815</td>\n",
       "      <td>0.819930</td>\n",
       "      <td>0.760774</td>\n",
       "      <td>0.691579</td>\n",
       "      <td>0.611855</td>\n",
       "      <td>...</td>\n",
       "      <td>0.831549</td>\n",
       "      <td>0.768205</td>\n",
       "      <td>0.710098</td>\n",
       "      <td>0.650517</td>\n",
       "      <td>0.645621</td>\n",
       "      <td>0.705880</td>\n",
       "      <td>0.775680</td>\n",
       "      <td>0.838907</td>\n",
       "      <td>0.856714</td>\n",
       "      <td>0.843591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.894249</td>\n",
       "      <td>0.799704</td>\n",
       "      <td>0.674077</td>\n",
       "      <td>0.664225</td>\n",
       "      <td>0.775689</td>\n",
       "      <td>0.861098</td>\n",
       "      <td>0.877680</td>\n",
       "      <td>0.876781</td>\n",
       "      <td>0.853368</td>\n",
       "      <td>0.834667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.949101</td>\n",
       "      <td>0.938460</td>\n",
       "      <td>0.910306</td>\n",
       "      <td>0.873802</td>\n",
       "      <td>0.856186</td>\n",
       "      <td>0.880806</td>\n",
       "      <td>0.903746</td>\n",
       "      <td>0.925508</td>\n",
       "      <td>0.941859</td>\n",
       "      <td>0.949329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>0.829815</td>\n",
       "      <td>0.832084</td>\n",
       "      <td>0.852396</td>\n",
       "      <td>0.909665</td>\n",
       "      <td>0.988242</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923323</td>\n",
       "      <td>0.821865</td>\n",
       "      <td>0.721302</td>\n",
       "      <td>0.612039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.844684</td>\n",
       "      <td>0.894573</td>\n",
       "      <td>0.941049</td>\n",
       "      <td>0.938311</td>\n",
       "      <td>0.889964</td>\n",
       "      <td>0.836136</td>\n",
       "      <td>0.782710</td>\n",
       "      <td>0.729510</td>\n",
       "      <td>0.713270</td>\n",
       "      <td>0.752194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>0.821040</td>\n",
       "      <td>0.803252</td>\n",
       "      <td>0.802413</td>\n",
       "      <td>0.808188</td>\n",
       "      <td>0.809180</td>\n",
       "      <td>0.796091</td>\n",
       "      <td>0.779687</td>\n",
       "      <td>0.778686</td>\n",
       "      <td>0.777388</td>\n",
       "      <td>0.796759</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986267</td>\n",
       "      <td>0.935172</td>\n",
       "      <td>0.905343</td>\n",
       "      <td>0.878278</td>\n",
       "      <td>0.859628</td>\n",
       "      <td>0.859715</td>\n",
       "      <td>0.854456</td>\n",
       "      <td>0.844587</td>\n",
       "      <td>0.832221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>0.834846</td>\n",
       "      <td>0.835026</td>\n",
       "      <td>0.848415</td>\n",
       "      <td>0.850183</td>\n",
       "      <td>0.852194</td>\n",
       "      <td>0.850544</td>\n",
       "      <td>0.852060</td>\n",
       "      <td>0.881185</td>\n",
       "      <td>0.913712</td>\n",
       "      <td>0.940417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859606</td>\n",
       "      <td>0.835307</td>\n",
       "      <td>0.760636</td>\n",
       "      <td>0.673133</td>\n",
       "      <td>0.572790</td>\n",
       "      <td>0.481443</td>\n",
       "      <td>0.410847</td>\n",
       "      <td>0.376488</td>\n",
       "      <td>0.351509</td>\n",
       "      <td>0.363247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>0.793255</td>\n",
       "      <td>0.703758</td>\n",
       "      <td>0.586116</td>\n",
       "      <td>0.458812</td>\n",
       "      <td>0.332085</td>\n",
       "      <td>0.216252</td>\n",
       "      <td>0.189161</td>\n",
       "      <td>0.293799</td>\n",
       "      <td>0.399837</td>\n",
       "      <td>0.506746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114903</td>\n",
       "      <td>0.120101</td>\n",
       "      <td>0.126889</td>\n",
       "      <td>0.131769</td>\n",
       "      <td>0.150811</td>\n",
       "      <td>0.163243</td>\n",
       "      <td>0.164040</td>\n",
       "      <td>0.159635</td>\n",
       "      <td>0.147002</td>\n",
       "      <td>0.134332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>0.929683</td>\n",
       "      <td>0.934702</td>\n",
       "      <td>0.922929</td>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.770322</td>\n",
       "      <td>0.674787</td>\n",
       "      <td>0.572909</td>\n",
       "      <td>0.491908</td>\n",
       "      <td>0.405543</td>\n",
       "      <td>0.378534</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833035</td>\n",
       "      <td>0.829597</td>\n",
       "      <td>0.830989</td>\n",
       "      <td>0.825764</td>\n",
       "      <td>0.825526</td>\n",
       "      <td>0.831062</td>\n",
       "      <td>0.851388</td>\n",
       "      <td>0.882273</td>\n",
       "      <td>0.922042</td>\n",
       "      <td>0.958114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>928 rows × 3059 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6     \\\n",
       "0    0.963402  0.966190  0.929274  0.865036  0.794078  0.704745  0.611444   \n",
       "1    0.541375  0.581143  0.620171  0.642222  0.669337  0.689173  0.766609   \n",
       "2    0.933871  0.933871  0.933871  0.933860  0.929573  0.902375  0.902041   \n",
       "3    0.840359  0.847049  0.852590  0.860626  0.857754  0.848815  0.819930   \n",
       "4    0.894249  0.799704  0.674077  0.664225  0.775689  0.861098  0.877680   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "923  0.829815  0.832084  0.852396  0.909665  0.988242  1.000000  0.923323   \n",
       "924  0.821040  0.803252  0.802413  0.808188  0.809180  0.796091  0.779687   \n",
       "925  0.834846  0.835026  0.848415  0.850183  0.852194  0.850544  0.852060   \n",
       "926  0.793255  0.703758  0.586116  0.458812  0.332085  0.216252  0.189161   \n",
       "927  0.929683  0.934702  0.922929  0.858824  0.770322  0.674787  0.572909   \n",
       "\n",
       "         7         8         9     ...      3050      3051      3052  \\\n",
       "0    0.513448  0.423232  0.368119  ...  0.527587  0.600228  0.668378   \n",
       "1    0.876878  0.942333  0.848699  ...  0.465567  0.467715  0.549389   \n",
       "2    0.869612  0.828913  0.814614  ...  0.949801  0.936326  0.922356   \n",
       "3    0.760774  0.691579  0.611855  ...  0.831549  0.768205  0.710098   \n",
       "4    0.876781  0.853368  0.834667  ...  0.949101  0.938460  0.910306   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "923  0.821865  0.721302  0.612039  ...  0.844684  0.894573  0.941049   \n",
       "924  0.778686  0.777388  0.796759  ...  1.000000  0.986267  0.935172   \n",
       "925  0.881185  0.913712  0.940417  ...  0.859606  0.835307  0.760636   \n",
       "926  0.293799  0.399837  0.506746  ...  0.114903  0.120101  0.126889   \n",
       "927  0.491908  0.405543  0.378534  ...  0.833035  0.829597  0.830989   \n",
       "\n",
       "         3053      3054      3055      3056      3057      3058      3059  \n",
       "0    0.732227  0.761643  0.749170  0.732649  0.754679  0.762857  0.754203  \n",
       "1    0.631177  0.690901  0.695485  0.662535  0.647105  0.659347  0.651568  \n",
       "2    0.929721  0.924299  0.922992  0.924178  0.932267  0.922647  0.899251  \n",
       "3    0.650517  0.645621  0.705880  0.775680  0.838907  0.856714  0.843591  \n",
       "4    0.873802  0.856186  0.880806  0.903746  0.925508  0.941859  0.949329  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "923  0.938311  0.889964  0.836136  0.782710  0.729510  0.713270  0.752194  \n",
       "924  0.905343  0.878278  0.859628  0.859715  0.854456  0.844587  0.832221  \n",
       "925  0.673133  0.572790  0.481443  0.410847  0.376488  0.351509  0.363247  \n",
       "926  0.131769  0.150811  0.163243  0.164040  0.159635  0.147002  0.134332  \n",
       "927  0.825764  0.825526  0.831062  0.851388  0.882273  0.922042  0.958114  \n",
       "\n",
       "[928 rows x 3059 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location= \"C:\\\\Users\\\\vargh\\\\Desktop\\\\Easy_G\\\\Combined_IDLead\\\\\"\n",
    "test_final = pd.DataFrame()\n",
    "for files in natsorted(os.listdir(location)):\n",
    "  if files.endswith(\".csv\") and not files.endswith(\"13.csv\"):\n",
    "    df=pd.read_csv(\"C:\\\\Users\\\\vargh\\\\Desktop\\\\Easy_G\\\\Combined_IDLead\\\\{}\".format(files))\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "      df.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "    test_final=pd.concat([test_final,df],axis=1,ignore_index=True)\n",
    "    test_final.drop(columns=test_final.columns[-1],axis=1,inplace=True)\n",
    "\n",
    "test_final.drop(columns=[255],axis=1,inplace=True)\n",
    "test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the file to csv\n",
    "test_final.to_csv('final_1D.csv',header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST DIMENSIONALITY REDUCTION EXPLAINED VARIANCE ON THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of each component: [8.42941573e-02 3.82496699e-02 3.73662558e-02 2.98682342e-02\n",
      " 2.51934716e-02 2.32945416e-02 2.09457640e-02 1.97572291e-02\n",
      " 1.93201383e-02 1.75039620e-02 1.60714000e-02 1.52444941e-02\n",
      " 1.45336193e-02 1.41812756e-02 1.33917803e-02 1.31039787e-02\n",
      " 1.25508820e-02 1.20236421e-02 1.17436624e-02 1.13879506e-02\n",
      " 1.07416627e-02 1.04947804e-02 1.02330757e-02 9.90774568e-03\n",
      " 9.81189616e-03 9.13396147e-03 9.04302230e-03 8.55432652e-03\n",
      " 8.45816777e-03 8.09155027e-03 7.97409753e-03 7.88191187e-03\n",
      " 7.30106078e-03 7.22314210e-03 7.08723187e-03 6.80458806e-03\n",
      " 6.66725267e-03 6.46645157e-03 6.22273530e-03 6.07444593e-03\n",
      " 6.05884172e-03 5.94271169e-03 5.83571348e-03 5.73882005e-03\n",
      " 5.55946220e-03 5.42688188e-03 5.20658443e-03 5.05093602e-03\n",
      " 5.00816627e-03 4.90397731e-03 4.80333824e-03 4.64331707e-03\n",
      " 4.55290422e-03 4.43018756e-03 4.29632786e-03 4.25589757e-03\n",
      " 4.12701943e-03 4.08357301e-03 4.02624940e-03 3.90456584e-03\n",
      " 3.87501083e-03 3.78687742e-03 3.77033198e-03 3.66785529e-03\n",
      " 3.59542939e-03 3.51829260e-03 3.41692522e-03 3.38419896e-03\n",
      " 3.32470378e-03 3.20194619e-03 3.15074343e-03 3.11849970e-03\n",
      " 3.09648855e-03 3.06486327e-03 3.02925284e-03 2.95705490e-03\n",
      " 2.86479047e-03 2.83935858e-03 2.81444802e-03 2.75625098e-03\n",
      " 2.71915237e-03 2.69585614e-03 2.60220359e-03 2.56630806e-03\n",
      " 2.53061239e-03 2.49968040e-03 2.45680407e-03 2.43798864e-03\n",
      " 2.40143320e-03 2.37368250e-03 2.37130876e-03 2.27657249e-03\n",
      " 2.25012665e-03 2.22066522e-03 2.18652877e-03 2.16073208e-03\n",
      " 2.13660395e-03 2.09739708e-03 2.08710501e-03 2.05329309e-03\n",
      " 2.01876288e-03 1.99306493e-03 1.94045580e-03 1.91300457e-03\n",
      " 1.89562565e-03 1.87900710e-03 1.82617680e-03 1.79957879e-03\n",
      " 1.78882734e-03 1.76950600e-03 1.73418380e-03 1.72463656e-03\n",
      " 1.71132051e-03 1.68664068e-03 1.67393949e-03 1.63850943e-03\n",
      " 1.63524394e-03 1.60217316e-03 1.55456062e-03 1.53901890e-03\n",
      " 1.52859225e-03 1.51576293e-03 1.50192166e-03 1.47784658e-03\n",
      " 1.46625964e-03 1.45717086e-03 1.43906428e-03 1.40059468e-03\n",
      " 1.38503941e-03 1.37814837e-03 1.36382162e-03 1.34928507e-03\n",
      " 1.32268201e-03 1.29140721e-03 1.28251506e-03 1.26632739e-03\n",
      " 1.26150178e-03 1.25106549e-03 1.23010595e-03 1.21972257e-03\n",
      " 1.21065226e-03 1.20208035e-03 1.18666073e-03 1.17409503e-03\n",
      " 1.15741150e-03 1.14665689e-03 1.13951166e-03 1.11453099e-03\n",
      " 1.11053248e-03 1.09310424e-03 1.09014039e-03 1.07132762e-03\n",
      " 1.05851716e-03 1.04929218e-03 1.03611371e-03 1.01609076e-03\n",
      " 1.01094759e-03 1.00218995e-03 9.94971167e-04 9.83218330e-04\n",
      " 9.77529479e-04 9.57530815e-04 9.45301293e-04 9.35389086e-04\n",
      " 9.19744824e-04 9.17784318e-04 9.07859671e-04 9.01902219e-04\n",
      " 8.88416363e-04 8.81315965e-04 8.74167812e-04 8.49646023e-04\n",
      " 8.43886089e-04 8.35780856e-04 8.27271682e-04 8.13579331e-04\n",
      " 8.04623143e-04 7.90238138e-04 7.84237317e-04 7.76293411e-04\n",
      " 7.66018609e-04 7.59530323e-04 7.53921909e-04 7.39502513e-04\n",
      " 7.28397063e-04 7.26679177e-04 7.18476634e-04 7.05826990e-04\n",
      " 7.01347959e-04 6.93161597e-04 6.88458643e-04 6.82652205e-04\n",
      " 6.78572363e-04 6.67104186e-04 6.66690720e-04 6.61302485e-04\n",
      " 6.48827849e-04 6.45271985e-04 6.33728413e-04 6.31633973e-04\n",
      " 6.26581147e-04 6.15610972e-04 6.07618709e-04 6.06396375e-04\n",
      " 5.95400747e-04 5.91346246e-04 5.87214234e-04 5.83192289e-04\n",
      " 5.76426768e-04 5.73094903e-04 5.63655596e-04 5.58873612e-04\n",
      " 5.57060413e-04 5.48808851e-04 5.47804436e-04 5.41343734e-04\n",
      " 5.31983518e-04 5.28358409e-04 5.21756368e-04 5.13324094e-04\n",
      " 5.12251998e-04 5.09511927e-04 5.01443500e-04 4.96198440e-04\n",
      " 4.93646892e-04 4.90699246e-04 4.82004572e-04 4.81857547e-04\n",
      " 4.78421032e-04 4.72227307e-04 4.66888083e-04 4.60732352e-04\n",
      " 4.56720904e-04 4.52763112e-04 4.48537485e-04 4.45847822e-04\n",
      " 4.42664774e-04 4.39468064e-04 4.31016453e-04 4.29997326e-04\n",
      " 4.25174846e-04 4.22028706e-04 4.18613571e-04 4.14387076e-04\n",
      " 4.07842254e-04 4.04434826e-04 4.03059305e-04 3.95563204e-04\n",
      " 3.95047671e-04 3.87046388e-04 3.84325063e-04 3.84222663e-04\n",
      " 3.78247813e-04 3.73250521e-04 3.66521142e-04 3.65368791e-04\n",
      " 3.61738321e-04 3.58866068e-04 3.54987554e-04 3.51399907e-04\n",
      " 3.47278763e-04 3.45611694e-04 3.42107860e-04 3.38452499e-04\n",
      " 3.33540315e-04 3.32763871e-04 3.30303572e-04 3.27944462e-04\n",
      " 3.23306499e-04 3.20110015e-04 3.16257767e-04 3.14935594e-04\n",
      " 3.09971373e-04 3.05917118e-04 3.04343912e-04 3.01535514e-04\n",
      " 2.99668363e-04 2.96032566e-04 2.93951434e-04 2.92882427e-04\n",
      " 2.90205024e-04 2.86423991e-04 2.84741505e-04 2.78091311e-04\n",
      " 2.75989056e-04 2.74117925e-04 2.71906029e-04 2.70087045e-04\n",
      " 2.67791883e-04 2.66678352e-04 2.60285828e-04 2.59165478e-04\n",
      " 2.56620162e-04 2.54315564e-04 2.53044545e-04 2.52061666e-04\n",
      " 2.44647349e-04 2.43073548e-04 2.41771583e-04 2.40514201e-04\n",
      " 2.37120820e-04 2.36233379e-04 2.34090251e-04 2.33011585e-04\n",
      " 2.30035484e-04 2.29529522e-04 2.23921827e-04 2.20958984e-04\n",
      " 2.18275429e-04 2.17595575e-04 2.15322572e-04 2.13333625e-04\n",
      " 2.10639100e-04 2.07695661e-04 2.06349023e-04 2.04656659e-04\n",
      " 2.00408247e-04 1.99566471e-04 1.98754591e-04 1.96790465e-04\n",
      " 1.94989591e-04 1.94304911e-04 1.92863558e-04 1.90061242e-04\n",
      " 1.86074116e-04 1.85336552e-04 1.82780684e-04 1.80844337e-04\n",
      " 1.78775491e-04 1.77757998e-04 1.75881171e-04 1.73658868e-04\n",
      " 1.71930157e-04 1.70379722e-04 1.68623915e-04 1.65829999e-04\n",
      " 1.64681773e-04 1.64343603e-04 1.62340850e-04 1.61683923e-04\n",
      " 1.60305784e-04 1.58930762e-04 1.55677552e-04 1.52181649e-04\n",
      " 1.51178554e-04 1.50300600e-04 1.49241701e-04 1.48909090e-04\n",
      " 1.47312402e-04 1.46399062e-04 1.42290801e-04 1.41941049e-04\n",
      " 1.40595963e-04 1.40213703e-04 1.38474771e-04 1.36628978e-04\n",
      " 1.35301259e-04 1.33243473e-04 1.32697087e-04 1.30280852e-04\n",
      " 1.29592399e-04 1.27551871e-04 1.27401618e-04 1.25496733e-04\n",
      " 1.24432274e-04 1.22726459e-04 1.21901300e-04 1.20793031e-04\n",
      " 1.19789754e-04 1.19095911e-04 1.18208873e-04 1.17002764e-04\n",
      " 1.16501436e-04 1.12893284e-04 1.12208538e-04 1.10208624e-04\n",
      " 1.09475047e-04 1.08723336e-04 1.07379784e-04 1.05848364e-04\n",
      " 1.04646907e-04 1.03649824e-04 1.01929730e-04 1.00398510e-04\n",
      " 9.95722085e-05 9.87158629e-05 9.72251133e-05 9.51608652e-05\n",
      " 9.36539420e-05 9.18660735e-05 9.10297449e-05 9.01517836e-05\n",
      " 8.91107083e-05 8.84624611e-05 8.75023225e-05 8.59342033e-05\n",
      " 8.34157310e-05 8.32641056e-05 8.23924844e-05 8.12474417e-05]\n",
      "\n",
      " Total Variance Explained: 99.54\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>391</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.109701</td>\n",
       "      <td>7.335801</td>\n",
       "      <td>-1.497240</td>\n",
       "      <td>3.744156</td>\n",
       "      <td>0.457898</td>\n",
       "      <td>0.533574</td>\n",
       "      <td>-2.690630</td>\n",
       "      <td>4.029798</td>\n",
       "      <td>2.639159</td>\n",
       "      <td>0.988393</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.184000</td>\n",
       "      <td>0.047102</td>\n",
       "      <td>-0.226289</td>\n",
       "      <td>-0.170925</td>\n",
       "      <td>0.081046</td>\n",
       "      <td>0.223417</td>\n",
       "      <td>-0.134093</td>\n",
       "      <td>0.170023</td>\n",
       "      <td>-0.069301</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.118849</td>\n",
       "      <td>-1.883519</td>\n",
       "      <td>5.106393</td>\n",
       "      <td>-1.067956</td>\n",
       "      <td>0.517365</td>\n",
       "      <td>-1.953456</td>\n",
       "      <td>1.973912</td>\n",
       "      <td>0.765298</td>\n",
       "      <td>-2.494711</td>\n",
       "      <td>0.463309</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007931</td>\n",
       "      <td>-0.055837</td>\n",
       "      <td>-0.103538</td>\n",
       "      <td>0.071883</td>\n",
       "      <td>0.059376</td>\n",
       "      <td>0.079544</td>\n",
       "      <td>0.005868</td>\n",
       "      <td>0.069370</td>\n",
       "      <td>-0.098837</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-7.413452</td>\n",
       "      <td>0.036968</td>\n",
       "      <td>-2.482773</td>\n",
       "      <td>1.462059</td>\n",
       "      <td>-0.461013</td>\n",
       "      <td>-3.331691</td>\n",
       "      <td>4.707335</td>\n",
       "      <td>-2.710678</td>\n",
       "      <td>-0.172138</td>\n",
       "      <td>-4.680300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106043</td>\n",
       "      <td>-0.162401</td>\n",
       "      <td>-0.015527</td>\n",
       "      <td>-0.026785</td>\n",
       "      <td>-0.078891</td>\n",
       "      <td>0.076339</td>\n",
       "      <td>0.075097</td>\n",
       "      <td>-0.163691</td>\n",
       "      <td>-0.119056</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4.057861</td>\n",
       "      <td>-0.067878</td>\n",
       "      <td>-0.135137</td>\n",
       "      <td>2.246169</td>\n",
       "      <td>-3.410695</td>\n",
       "      <td>4.386389</td>\n",
       "      <td>-0.388125</td>\n",
       "      <td>-0.798696</td>\n",
       "      <td>0.453611</td>\n",
       "      <td>-2.257288</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.154846</td>\n",
       "      <td>0.094212</td>\n",
       "      <td>0.196916</td>\n",
       "      <td>0.186032</td>\n",
       "      <td>-0.031161</td>\n",
       "      <td>-0.031502</td>\n",
       "      <td>-0.195149</td>\n",
       "      <td>-0.037490</td>\n",
       "      <td>-0.218652</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-4.916389</td>\n",
       "      <td>-1.210403</td>\n",
       "      <td>-2.453160</td>\n",
       "      <td>4.643236</td>\n",
       "      <td>1.160080</td>\n",
       "      <td>-5.017612</td>\n",
       "      <td>-0.268278</td>\n",
       "      <td>-0.536101</td>\n",
       "      <td>-3.010770</td>\n",
       "      <td>3.432581</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.168386</td>\n",
       "      <td>0.066845</td>\n",
       "      <td>0.052380</td>\n",
       "      <td>-0.064948</td>\n",
       "      <td>0.074353</td>\n",
       "      <td>-0.111654</td>\n",
       "      <td>-0.037540</td>\n",
       "      <td>-0.124845</td>\n",
       "      <td>0.138638</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>-4.912314</td>\n",
       "      <td>-2.995334</td>\n",
       "      <td>-2.268745</td>\n",
       "      <td>3.747290</td>\n",
       "      <td>1.722749</td>\n",
       "      <td>1.811304</td>\n",
       "      <td>1.201766</td>\n",
       "      <td>-0.282246</td>\n",
       "      <td>0.536166</td>\n",
       "      <td>-0.164191</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047474</td>\n",
       "      <td>-0.067825</td>\n",
       "      <td>0.039634</td>\n",
       "      <td>0.036562</td>\n",
       "      <td>-0.057192</td>\n",
       "      <td>0.026914</td>\n",
       "      <td>-0.064353</td>\n",
       "      <td>-0.211302</td>\n",
       "      <td>-0.173208</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>-1.088166</td>\n",
       "      <td>4.360554</td>\n",
       "      <td>0.651453</td>\n",
       "      <td>-3.107551</td>\n",
       "      <td>2.723604</td>\n",
       "      <td>2.042871</td>\n",
       "      <td>2.538168</td>\n",
       "      <td>0.850741</td>\n",
       "      <td>3.283104</td>\n",
       "      <td>-3.649345</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041725</td>\n",
       "      <td>0.163294</td>\n",
       "      <td>-0.026512</td>\n",
       "      <td>-0.237879</td>\n",
       "      <td>-0.040446</td>\n",
       "      <td>0.296463</td>\n",
       "      <td>-0.129475</td>\n",
       "      <td>0.137569</td>\n",
       "      <td>0.194417</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>5.798872</td>\n",
       "      <td>-0.960964</td>\n",
       "      <td>-1.807671</td>\n",
       "      <td>-2.260827</td>\n",
       "      <td>-0.510133</td>\n",
       "      <td>1.988053</td>\n",
       "      <td>-1.470387</td>\n",
       "      <td>-1.357249</td>\n",
       "      <td>2.241718</td>\n",
       "      <td>-2.662326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024909</td>\n",
       "      <td>0.123600</td>\n",
       "      <td>-0.064419</td>\n",
       "      <td>0.037547</td>\n",
       "      <td>-0.114848</td>\n",
       "      <td>0.114887</td>\n",
       "      <td>-0.149419</td>\n",
       "      <td>0.009093</td>\n",
       "      <td>0.093262</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>11.855129</td>\n",
       "      <td>4.813181</td>\n",
       "      <td>-0.065306</td>\n",
       "      <td>-1.386291</td>\n",
       "      <td>1.143359</td>\n",
       "      <td>0.315840</td>\n",
       "      <td>0.073259</td>\n",
       "      <td>1.525808</td>\n",
       "      <td>-0.161116</td>\n",
       "      <td>1.089716</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077333</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>-0.081739</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.082041</td>\n",
       "      <td>0.200737</td>\n",
       "      <td>0.141105</td>\n",
       "      <td>-0.089866</td>\n",
       "      <td>-0.108731</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>-0.658972</td>\n",
       "      <td>3.101759</td>\n",
       "      <td>-2.566358</td>\n",
       "      <td>-0.374202</td>\n",
       "      <td>-0.586525</td>\n",
       "      <td>-1.297507</td>\n",
       "      <td>-3.543194</td>\n",
       "      <td>0.589991</td>\n",
       "      <td>-1.248622</td>\n",
       "      <td>-1.648601</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064927</td>\n",
       "      <td>-0.047573</td>\n",
       "      <td>0.003913</td>\n",
       "      <td>0.131860</td>\n",
       "      <td>-0.023310</td>\n",
       "      <td>0.130877</td>\n",
       "      <td>-0.053821</td>\n",
       "      <td>0.009326</td>\n",
       "      <td>-0.086169</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>928 rows × 401 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     3.109701  7.335801 -1.497240  3.744156  0.457898  0.533574 -2.690630   \n",
       "1     0.118849 -1.883519  5.106393 -1.067956  0.517365 -1.953456  1.973912   \n",
       "2    -7.413452  0.036968 -2.482773  1.462059 -0.461013 -3.331691  4.707335   \n",
       "3    -4.057861 -0.067878 -0.135137  2.246169 -3.410695  4.386389 -0.388125   \n",
       "4    -4.916389 -1.210403 -2.453160  4.643236  1.160080 -5.017612 -0.268278   \n",
       "..         ...       ...       ...       ...       ...       ...       ...   \n",
       "923  -4.912314 -2.995334 -2.268745  3.747290  1.722749  1.811304  1.201766   \n",
       "924  -1.088166  4.360554  0.651453 -3.107551  2.723604  2.042871  2.538168   \n",
       "925   5.798872 -0.960964 -1.807671 -2.260827 -0.510133  1.988053 -1.470387   \n",
       "926  11.855129  4.813181 -0.065306 -1.386291  1.143359  0.315840  0.073259   \n",
       "927  -0.658972  3.101759 -2.566358 -0.374202 -0.586525 -1.297507 -3.543194   \n",
       "\n",
       "            7         8         9  ...       391       392       393  \\\n",
       "0    4.029798  2.639159  0.988393  ... -0.184000  0.047102 -0.226289   \n",
       "1    0.765298 -2.494711  0.463309  ...  0.007931 -0.055837 -0.103538   \n",
       "2   -2.710678 -0.172138 -4.680300  ...  0.106043 -0.162401 -0.015527   \n",
       "3   -0.798696  0.453611 -2.257288  ... -0.154846  0.094212  0.196916   \n",
       "4   -0.536101 -3.010770  3.432581  ... -0.168386  0.066845  0.052380   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "923 -0.282246  0.536166 -0.164191  ... -0.047474 -0.067825  0.039634   \n",
       "924  0.850741  3.283104 -3.649345  ...  0.041725  0.163294 -0.026512   \n",
       "925 -1.357249  2.241718 -2.662326  ...  0.024909  0.123600 -0.064419   \n",
       "926  1.525808 -0.161116  1.089716  ...  0.077333  0.002783 -0.081739   \n",
       "927  0.589991 -1.248622 -1.648601  ...  0.064927 -0.047573  0.003913   \n",
       "\n",
       "          394       395       396       397       398       399  target  \n",
       "0   -0.170925  0.081046  0.223417 -0.134093  0.170023 -0.069301       2  \n",
       "1    0.071883  0.059376  0.079544  0.005868  0.069370 -0.098837       2  \n",
       "2   -0.026785 -0.078891  0.076339  0.075097 -0.163691 -0.119056       2  \n",
       "3    0.186032 -0.031161 -0.031502 -0.195149 -0.037490 -0.218652       2  \n",
       "4   -0.064948  0.074353 -0.111654 -0.037540 -0.124845  0.138638       2  \n",
       "..        ...       ...       ...       ...       ...       ...     ...  \n",
       "923  0.036562 -0.057192  0.026914 -0.064353 -0.211302 -0.173208       3  \n",
       "924 -0.237879 -0.040446  0.296463 -0.129475  0.137569  0.194417       3  \n",
       "925  0.037547 -0.114848  0.114887 -0.149419  0.009093  0.093262       3  \n",
       "926  0.001818  0.082041  0.200737  0.141105 -0.089866 -0.108731       3  \n",
       "927  0.131860 -0.023310  0.130877 -0.053821  0.009326 -0.086169       3  \n",
       "\n",
       "[928 rows x 401 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now Perform Dimensionality reduction (PCA) on that Dataframe and check\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#do PCA and choose componeents as 400\n",
    "pca = PCA(n_components=400)\n",
    "x_pca = pca.fit_transform(test_final)\n",
    "x_pca = pd.DataFrame(x_pca)\n",
    "\n",
    "# Calculate the variance explained by priciple components\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print('Variance of each component:', pca.explained_variance_ratio_)\n",
    "print('\\n Total Variance Explained:', round(sum(list(pca.explained_variance_ratio_))*100, 2))\n",
    "\n",
    "#store the new pca generated dimensions in a dataframe\n",
    "#store the new pca generated dimensions in a dataframe\n",
    "pca_df = pd.DataFrame(data = x_pca)\n",
    "target = pd.Series(result_df.iloc[:,-1], name='target')\n",
    "final_result_df = pd.concat([pca_df, target], axis=1)\n",
    "final_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to dimensionally reduced csv file\n",
    "final_result_df.to_csv(\"pca_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PCA_ECG.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "#save the PCA model\n",
    "joblib_file='PCA_ECG.pkl'\n",
    "joblib.dump(pca,joblib_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRYING DIFFERENT ML MODELS ON THE ALL 12 LEADS COMBINED FILE WITHOUT DIMENSIONALITY REDUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8440860215053764\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.71      0.81       105\n",
      "           1       0.94      1.00      0.97        94\n",
      "           2       0.80      0.89      0.84       112\n",
      "           3       0.68      0.74      0.71        61\n",
      "\n",
      "    accuracy                           0.84       372\n",
      "   macro avg       0.84      0.84      0.83       372\n",
      "weighted avg       0.85      0.84      0.84       372\n",
      "\n",
      "Tuned Model Parameters: {'knn__n_neighbors': 1}\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules for ML model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Setup the pipeline steps:\n",
    "steps = [('knn', KNeighborsClassifier())]\n",
    "        \n",
    "# Create the pipeline: pipeline \n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# have paased less range value of hyperparamter since i'm using free tier version of google colab.\n",
    "k_range = list(range(1, 30))\n",
    "parameters = dict(knn__n_neighbors=k_range)\n",
    "\n",
    "#input\n",
    "X = pd.read_csv('final_1D.csv',header=None)\n",
    "\n",
    "#target\n",
    "y=final_result_df.iloc[:,-1]\n",
    "\n",
    "# Create train and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n",
    "\n",
    "#increasing cv score takes lot of time in gooogle colab, so kept it just 2.\n",
    "cv = GridSearchCV(pipeline,parameters,cv=2)\n",
    "\n",
    "cv.fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = cv.predict(X_test)\n",
    "\n",
    "Knn_Accuracy = cv.score(X_test, y_test)\n",
    "\n",
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(Knn_Accuracy))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8118279569892473\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.65      0.72       105\n",
      "           1       0.90      1.00      0.95        94\n",
      "           2       0.78      0.88      0.83       112\n",
      "           3       0.71      0.69      0.70        61\n",
      "\n",
      "    accuracy                           0.81       372\n",
      "   macro avg       0.80      0.80      0.80       372\n",
      "weighted avg       0.81      0.81      0.81       372\n",
      "\n",
      "Tuned Model Parameters: {'lr__C': 0.046415888336127774, 'lr__penalty': 'l2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Setup the pipeline steps:\n",
    "steps = [('lr', LogisticRegression())]\n",
    "        \n",
    "# Create the pipeline: pipeline \n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "#input\n",
    "X = pd.read_csv('final_1D.csv',header=None)\n",
    "\n",
    "#target\n",
    "y=final_result_df.iloc[:,-1]\n",
    "\n",
    "#parameters for gridsearchcv\n",
    "c_space = np.logspace(-4, 4, 10)\n",
    "parameters = {'lr__C': c_space,'lr__penalty': ['l2']} \n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n",
    "\n",
    "#call GridSearchCV and set crossvalscore to 2\n",
    "cv = GridSearchCV(pipeline,parameters,cv=2)\n",
    "\n",
    "cv.fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = cv.predict(X_test)\n",
    "LR_Accuracy = cv.score(X_test, y_test)\n",
    "\n",
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(LR_Accuracy))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8922413793103449\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.90      0.86       119\n",
      "           1       1.00      1.00      1.00       125\n",
      "           2       0.84      0.93      0.88       140\n",
      "           3       0.96      0.65      0.78        80\n",
      "\n",
      "    accuracy                           0.89       464\n",
      "   macro avg       0.91      0.87      0.88       464\n",
      "weighted avg       0.90      0.89      0.89       464\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules for ML model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Setup the pipeline\n",
    "steps = [('SVM', SVC())]\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "#input\n",
    "X = pd.read_csv('final_1D.csv',header=None)\n",
    "\n",
    "#target\n",
    "y=final_result_df.iloc[:,-1]\n",
    "\n",
    "# Specify the hyperparameter space, if we increase the penalty(c) and gamma value the accurancy can be increased.\n",
    "#since it takes lots of time in google colab provided only a single value\n",
    "parameters = {'SVM__C':[1, 10, 100],\n",
    "              'SVM__gamma':[0.1, 0.01]}\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.5,random_state=21)\n",
    "\n",
    "cv = GridSearchCV(pipeline,parameters,cv=3)\n",
    "cv.fit(X_train,y_train)\n",
    "\n",
    "y_pred = cv.predict(X_test)\n",
    "SVM_Accuracy = cv.score(X_test, y_test)\n",
    "\n",
    "# Compute and print metrics\n",
    "SVM_Accuracy=cv.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy: {}\".format(SVM_Accuracy))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;XGBClassifier<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;, ...)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, objective='multi:softprob', ...)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8254310344827587\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.62      0.69       119\n",
      "           1       0.97      1.00      0.98       125\n",
      "           2       0.77      0.91      0.84       140\n",
      "           3       0.77      0.70      0.73        80\n",
      "\n",
      "    accuracy                           0.83       464\n",
      "   macro avg       0.82      0.81      0.81       464\n",
      "weighted avg       0.82      0.83      0.82       464\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: {}\".format(accuracy))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVING A VERY BASIC ML MODEL AND USING IT ON REALTIME PIPELINE TO CHECK WORKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knn_model_test.pkl']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the necessary modules for ML model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import joblib\n",
    "#input\n",
    "X = final_result_df.iloc[:,:-1]\n",
    "#target\n",
    "y=final_result_df.iloc[:,-1]\n",
    "\n",
    "# Create train and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n",
    "\n",
    "knn =  KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "joblib_file='knn_model_test.pkl'\n",
    "joblib.dump(knn,joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_model_test.pkl']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the necessary modules for ML model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "#input\n",
    "X = pd.read_csv('final_1D.csv',header=None)\n",
    "\n",
    "#target\n",
    "y=final_result_df.iloc[:,-1]\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.5,random_state=21)\n",
    "\n",
    "svm=SVC(C=10,gamma=0.01)\n",
    "\n",
    "svm.fit(X_train,y_train)\n",
    "\n",
    "joblib_file='svm_model_test.pkl'\n",
    "joblib.dump(svm,joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.68      0.79       105\n",
      "           1       0.94      1.00      0.97        94\n",
      "           2       0.80      0.89      0.84       112\n",
      "           3       0.67      0.79      0.72        61\n",
      "\n",
      "    accuracy                           0.84       372\n",
      "   macro avg       0.84      0.84      0.83       372\n",
      "weighted avg       0.85      0.84      0.84       372\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Read the original data\n",
    "X = pd.read_csv('final_1D.csv', header=None)\n",
    "\n",
    "# Generate synthetic features\n",
    "num_samples = X.shape[0]  # Number of samples\n",
    "num_additional_features = 1  # Number of additional features\n",
    "\n",
    "# Generate random values for the synthetic features\n",
    "synthetic_features = pd.DataFrame(np.random.randn(num_samples, num_additional_features))\n",
    "\n",
    "# Concatenate the synthetic features with the original data\n",
    "X = pd.concat([X, synthetic_features], axis=1)\n",
    "\n",
    "# Assuming final_result_df is defined somewhere in your code\n",
    "y = final_result_df.iloc[:, -1]\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Initialize KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# Train the classifier\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Save the trained model\n",
    "joblib_file = 'model_test.pkl'\n",
    "joblib.dump(knn, joblib_file)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENSEMBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required modules\n",
    "from sklearn import linear_model, tree, ensemble\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input\n",
    "X = final_result_df.iloc[:,0:-1]\n",
    "\n",
    "#target\n",
    "y=final_result_df.iloc[:,-1]\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking of ML Models\n",
    "eclf = VotingClassifier(estimators=[ \n",
    "    ('SVM', SVC(probability=True)),\n",
    "    ('knn', KNeighborsClassifier()),\n",
    "    ('rf', ensemble.RandomForestClassifier()),\n",
    "    ('bayes',GaussianNB()),\n",
    "    ('logistic',LogisticRegression()),\n",
    "    ], voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 9\u001b[0m\n\u001b[0;32m      2\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSVM__C\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m100\u001b[39m],\n\u001b[0;32m      3\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSVM__gamma\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.01\u001b[39m],\n\u001b[0;32m      4\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mknn__n_neighbors\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m5\u001b[39m],\n\u001b[0;32m      5\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrf__n_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;241m300\u001b[39m, \u001b[38;5;241m400\u001b[39m],\n\u001b[0;32m      6\u001b[0m           }\n\u001b[0;32m      8\u001b[0m grid \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39meclf, param_grid\u001b[38;5;241m=\u001b[39mparams, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m voting_clf \u001b[38;5;241m=\u001b[39m \u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(grid\u001b[38;5;241m.\u001b[39mbest_params_)\n\u001b[0;32m     12\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m voting_clf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    966\u001b[0m     )\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1527\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    913\u001b[0m         )\n\u001b[0;32m    914\u001b[0m     )\n\u001b[1;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    939\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:895\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    893\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    894\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 895\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    898\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    899\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:366\u001b[0m, in \u001b[0;36mVotingClassifier.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mle_\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m    364\u001b[0m transformed_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mle_\u001b[38;5;241m.\u001b[39mtransform(y)\n\u001b[1;32m--> 366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:89\u001b[0m, in \u001b[0;36m_BaseVoting.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators):\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of `estimators` and weights must be equal; got\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m weights, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     87\u001b[0m     )\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_single_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVoting\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclfs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclfs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    100\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_estimators_ \u001b[38;5;241m=\u001b[39m Bunch()\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Uses 'drop' as placeholder for dropped estimators\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:36\u001b[0m, in \u001b[0;36m_fit_single_estimator\u001b[1;34m(estimator, X, y, sample_weight, message_clsname, message)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m---> 36\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m estimator\n",
      "File \u001b[1;32mc:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    481\u001b[0m ]\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 192\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    201\u001b[0m         X,\n\u001b[0;32m    202\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    206\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\vargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameter Tuning using gridSearch\n",
    "params = {'SVM__C':[1, 10, 100],\n",
    "          'SVM__gamma':[0.1, 0.01],\n",
    "          'knn__n_neighbors': [1,3,5],\n",
    "          'rf__n_estimators':[300, 400],\n",
    "          }\n",
    "\n",
    "grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\n",
    "voting_clf = grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "# Compute and print metrics\n",
    "Voting_Accuracy=voting_clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy: {}\".format(Voting_Accuracy))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(voting_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHeart_Disease_Prediction_using_ECG.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# dump information to that file\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mpickle\u001b[49m\u001b[38;5;241m.\u001b[39mdump(voting_clf, file)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "# open a file, where you ant to store the data\n",
    "file = open('Heart_Disease_Prediction_using_ECG.pkl', 'wb')\n",
    "# dump information to that file\n",
    "pickle.dump(voting_clf, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
